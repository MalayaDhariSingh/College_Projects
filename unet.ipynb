{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNokIViM+y8/NKwainPHJuM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7WIE8aNmg3hG","executionInfo":{"status":"ok","timestamp":1763284307670,"user_tz":-330,"elapsed":14711,"user":{"displayName":"Malaya Dhari Singh","userId":"06362993853657401156"}}},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","\n","class CarvanaDataset(Dataset):\n","    def __init__(self, root_path, test=False):\n","        self.root_path = root_path\n","        if test:\n","            self.images = sorted([root_path+\"/manual_test/\"+i for i in os.listdir(root_path+\"/manual_test/\")])\n","            self.masks = sorted([root_path+\"/manual_test_masks/\"+i for i in os.listdir(root_path+\"/manual_test_masks/\")])\n","        else:\n","            self.images = sorted([root_path+\"/train/\"+i for i in os.listdir(root_path+\"/train/\")])\n","            self.masks = sorted([root_path+\"/train_masks/\"+i for i in os.listdir(root_path+\"/train_masks/\")])\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((512, 512)),\n","            transforms.ToTensor()])\n","\n","    def __getitem__(self, index):\n","        img = Image.open(self.images[index]).convert(\"RGB\")\n","        mask = Image.open(self.masks[index]).convert(\"L\")\n","\n","        return self.transform(img), self.transform(mask)\n","\n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from PIL import Image\n","\n","from carvana_dataset import CarvanaDataset\n","from unet import UNet\n","\n","def pred_show_image_grid(data_path, model_pth, device):\n","    model = UNet(in_channels=3, num_classes=1).to(device)\n","    model.load_state_dict(torch.load(model_pth, map_location=torch.device(device)))\n","    image_dataset = CarvanaDataset(data_path, test=True)\n","    images = []\n","    orig_masks = []\n","    pred_masks = []\n","\n","    for img, orig_mask in image_dataset:\n","        img = img.float().to(device)\n","        img = img.unsqueeze(0)\n","\n","        pred_mask = model(img)\n","\n","        img = img.squeeze(0).cpu().detach()\n","        img = img.permute(1, 2, 0)\n","\n","        pred_mask = pred_mask.squeeze(0).cpu().detach()\n","        pred_mask = pred_mask.permute(1, 2, 0)\n","        pred_mask[pred_mask < 0]=0\n","        pred_mask[pred_mask > 0]=1\n","\n","        orig_mask = orig_mask.cpu().detach()\n","        orig_mask = orig_mask.permute(1, 2, 0)\n","\n","        images.append(img)\n","        orig_masks.append(orig_mask)\n","        pred_masks.append(pred_mask)\n","\n","    images.extend(orig_masks)\n","    images.extend(pred_masks)\n","    fig = plt.figure()\n","    for i in range(1, 3*len(image_dataset)+1):\n","       fig.add_subplot(3, len(image_dataset), i)\n","       plt.imshow(images[i-1], cmap=\"gray\")\n","    plt.show()\n","\n","\n","def single_image_inference(image_pth, model_pth, device):\n","    model = UNet(in_channels=3, num_classes=1).to(device)\n","    model.load_state_dict(torch.load(model_pth, map_location=torch.device(device)))\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((512, 512)),\n","        transforms.ToTensor()])\n","\n","    img = transform(Image.open(image_pth)).float().to(device)\n","    img = img.unsqueeze(0)\n","\n","    pred_mask = model(img)\n","\n","    img = img.squeeze(0).cpu().detach()\n","    img = img.permute(1, 2, 0)\n","\n","    pred_mask = pred_mask.squeeze(0).cpu().detach()\n","    pred_mask = pred_mask.permute(1, 2, 0)\n","    pred_mask[pred_mask < 0]=0\n","    pred_mask[pred_mask > 0]=1\n","\n","    fig = plt.figure()\n","    for i in range(1, 3):\n","        fig.add_subplot(1, 2, i)\n","        if i == 1:\n","            plt.imshow(img, cmap=\"gray\")\n","        else:\n","            plt.imshow(pred_mask, cmap=\"gray\")\n","    plt.show()\n","\n","\n","if __name__ == \"__main__\":\n","    SINGLE_IMG_PATH = \"./data/manual_test/03a857ce842d_15.jpg\"\n","    DATA_PATH = \"./data\"\n","    MODEL_PATH = \"./models/unet.pth\"\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    pred_show_image_grid(DATA_PATH, MODEL_PATH, device)\n","    single_image_inference(SINGLE_IMG_PATH, MODEL_PATH, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"collapsed":true,"id":"JT1D-5OE7qOC","executionInfo":{"status":"error","timestamp":1763284338636,"user_tz":-330,"elapsed":430,"user":{"displayName":"Malaya Dhari Singh","userId":"06362993853657401156"}},"outputId":"4c029b23-3a3f-46e7-f893-eba2e56406e8"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'carvana_dataset'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2058348449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcarvana_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarvanaDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'carvana_dataset'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import torch\n","from torch import optim, nn\n","from torch.utils.data import DataLoader, random_split\n","from tqdm import tqdm\n","\n","from unet import UNet\n","from carvana_dataset import CarvanaDataset\n","\n","if __name__ == \"__main__\":\n","    LEARNING_RATE = 3e-4\n","    BATCH_SIZE = 32\n","    EPOCHS = 2\n","    DATA_PATH = \"/content/drive/MyDrive/uygar/unet-segmentation/data\"\n","    MODEL_SAVE_PATH = \"/content/drive/MyDrive/uygar/unet-segmentation/models/unet.pth\"\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    train_dataset = CarvanaDataset(DATA_PATH)\n","\n","    generator = torch.Generator().manual_seed(42)\n","    train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2], generator=generator)\n","\n","    train_dataloader = DataLoader(dataset=train_dataset,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=True)\n","    val_dataloader = DataLoader(dataset=val_dataset,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=True)\n","\n","    model = UNet(in_channels=3, num_classes=1).to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    for epoch in tqdm(range(EPOCHS)):\n","        model.train()\n","        train_running_loss = 0\n","        for idx, img_mask in enumerate(tqdm(train_dataloader)):\n","            img = img_mask[0].float().to(device)\n","            mask = img_mask[1].float().to(device)\n","\n","            y_pred = model(img)\n","            optimizer.zero_grad()\n","\n","            loss = criterion(y_pred, mask)\n","            train_running_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        train_loss = train_running_loss / (idx + 1)\n","\n","        model.eval()\n","        val_running_loss = 0\n","        with torch.no_grad():\n","            for idx, img_mask in enumerate(tqdm(val_dataloader)):\n","                img = img_mask[0].float().to(device)\n","                mask = img_mask[1].float().to(device)\n","\n","                y_pred = model(img)\n","                loss = criterion(y_pred, mask)\n","\n","                val_running_loss += loss.item()\n","\n","            val_loss = val_running_loss / (idx + 1)\n","\n","        print(\"-\"*30)\n","        print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n","        print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n","        print(\"-\"*30)\n","\n","    torch.save(model.state_dict(), MODEL_SAVE_PATH)"],"metadata":{"id":"T4Z5KXaB78D-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","from unet_parts import DoubleConv, DownSample, UpSample\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.down_convolution_1 = DownSample(in_channels, 64)\n","        self.down_convolution_2 = DownSample(64, 128)\n","        self.down_convolution_3 = DownSample(128, 256)\n","        self.down_convolution_4 = DownSample(256, 512)\n","\n","        self.bottle_neck = DoubleConv(512, 1024)\n","\n","        self.up_convolution_1 = UpSample(1024, 512)\n","        self.up_convolution_2 = UpSample(512, 256)\n","        self.up_convolution_3 = UpSample(256, 128)\n","        self.up_convolution_4 = UpSample(128, 64)\n","\n","        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","       down_1, p1 = self.down_convolution_1(x)\n","       down_2, p2 = self.down_convolution_2(p1)\n","       down_3, p3 = self.down_convolution_3(p2)\n","       down_4, p4 = self.down_convolution_4(p3)\n","\n","       b = self.bottle_neck(p4)\n","\n","       up_1 = self.up_convolution_1(b, down_4)\n","       up_2 = self.up_convolution_2(up_1, down_3)\n","       up_3 = self.up_convolution_3(up_2, down_2)\n","       up_4 = self.up_convolution_4(up_3, down_1)\n","\n","       out = self.out(up_4)\n","       return out"],"metadata":{"id":"ZQXXT07B8Owv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv_op = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_op(x)\n","\n","\n","class DownSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = DoubleConv(in_channels, out_channels)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        down = self.conv(x)\n","        p = self.pool(down)\n","\n","        return down, p\n","\n","\n","class UpSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","       x1 = self.up(x1)\n","       x = torch.cat([x1, x2], 1)\n","       return self.conv(x)"],"metadata":{"id":"-EbxsQsa8U9m"},"execution_count":null,"outputs":[]}]}