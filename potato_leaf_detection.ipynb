{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMi+OjyrrWYi8kCuCyyeWND"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bayPVtEsAQyw"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import sys\n","\n","# Configuration\n","IMG_SIZE = 256\n","MODEL_PATH = \"models/potato_disease_model.h5\"\n","CLASS_NAMES = ['Early_Blight', 'Late_Blight', 'Healthy']\n","\n","def load_and_preprocess_image(image_path):\n","    \"\"\"Load and preprocess a single image\"\"\"\n","    img = Image.open(image_path)\n","    img = img.resize((IMG_SIZE, IMG_SIZE))\n","    img_array = np.array(img)\n","    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n","    return img_array, img\n","\n","def predict_disease(image_path, model):\n","    \"\"\"Predict disease from image\"\"\"\n","    img_array, original_img = load_and_preprocess_image(image_path)\n","\n","    predictions = model.predict(img_array)\n","    predicted_class = CLASS_NAMES[np.argmax(predictions[0])]\n","    confidence = np.max(predictions[0]) * 100\n","\n","    return predicted_class, confidence, predictions[0], original_img\n","\n","def display_prediction(image_path, predicted_class, confidence, predictions, original_img):\n","    \"\"\"Display image with prediction results\"\"\"\n","    plt.figure(figsize=(12, 5))\n","\n","    # Display image\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(original_img)\n","    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}%\")\n","    plt.axis('off')\n","\n","    # Display prediction probabilities\n","    plt.subplot(1, 2, 2)\n","    plt.barh(CLASS_NAMES, predictions * 100)\n","    plt.xlabel('Confidence (%)')\n","    plt.title('Prediction Probabilities')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def main():\n","    if len(sys.argv) < 2:\n","        print(\"Usage: python predict.py <image_path>\")\n","        print(\"Example: python predict.py data/test/sample_leaf.jpg\")\n","        return\n","\n","    image_path = sys.argv[1]\n","\n","    print(\"Loading model...\")\n","    model = tf.keras.models.load_model(MODEL_PATH)\n","\n","    print(f\"Predicting disease for: {image_path}\")\n","    predicted_class, confidence, predictions, original_img = predict_disease(image_path, model)\n","\n","    print(f\"\\n{'='*50}\")\n","    print(f\"Prediction: {predicted_class}\")\n","    print(f\"Confidence: {confidence:.2f}%\")\n","    print(f\"{'='*50}\")\n","    print(\"\\nAll class probabilities:\")\n","    for class_name, prob in zip(CLASS_NAMES, predictions):\n","        print(f\"  {class_name}: {prob*100:.2f}%\")\n","\n","    # Display visualization\n","    display_prediction(image_path, predicted_class, confidence, predictions, original_img)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","\n","# --- Configuration Constants ---\n","# These should match your training script\n","IMG_SIZE = 256\n","BATCH_SIZE = 32\n","SEED = 42  # for reproducibility\n","VAL_SPLIT = 0.2  # 20% of training data for validation\n","TRAIN_DIR = \"data/train\"\n","TEST_DIR = \"data/test\" # Optional: if you have a separate test set\n","\n","\n","def build_augmentation_layer():\n","    \"\"\"\n","    Creates a Sequential model with data augmentation layers.\n","    These augmentations are applied randomly to the training images\n","    to help prevent overfitting.\n","    \"\"\"\n","    return tf.keras.Sequential([\n","        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=SEED),\n","        tf.keras.layers.RandomRotation(0.2, seed=SEED),\n","        tf.keras.layers.RandomZoom(0.2, seed=SEED),\n","        tf.keras.layers.RandomContrast(0.2, seed=SEED),\n","    ], name=\"data_augmentation\")\n","\n","\n","def load_datasets(train_dir=TRAIN_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE, val_split=VAL_SPLIT):\n","    \"\"\"\n","    Loads, splits, and preprocesses the training and validation datasets.\n","\n","    Uses `image_dataset_from_directory` to efficiently load images from\n","    the folder structure (e.g., data/train/Healthy, data/train/Late_Blight).\n","\n","    Returns:\n","        tuple: (train_ds, val_ds, class_names)\n","    \"\"\"\n","    print(f\"Loading training and validation data from: {train_dir}\")\n","\n","    # Create the training dataset\n","    train_ds = tf.keras.utils.image_dataset_from_directory(\n","        train_dir,\n","        label_mode='categorical',  # Assumes multi-class classification\n","        validation_split=val_split,\n","        subset=\"training\",\n","        seed=SEED,\n","        image_size=(img_size, img_size),\n","        batch_size=batch_size\n","    )\n","\n","    # Create the validation dataset\n","    val_ds = tf.keras.utils.image_dataset_from_directory(\n","        train_dir,\n","        label_mode='categorical',\n","        validation_split=val_split,\n","        subset=\"validation\",\n","        seed=SEED,\n","        image_size=(img_size, img_size),\n","        batch_size=batch_size\n","    )\n","\n","    class_names = train_ds.class_names\n","    print(f\"Classes found: {class_names}\")\n","\n","    return train_ds, val_ds, class_names\n","\n","\n","def load_test_dataset(test_dir=TEST_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n","    \"\"\"\n","    Loads the test dataset.\n","    Assumes it's in a similar folder structure or a single folder.\n","    If no subfolders, set `labels=None`.\n","    \"\"\"\n","    if not os.path.exists(test_dir):\n","        print(f\"Test directory not found: {test_dir}. Skipping test set.\")\n","        return None\n","\n","    print(f\"Loading test data from: {test_dir}\")\n","    test_ds = tf.keras.utils.image_dataset_from_directory(\n","        test_dir,\n","        label_mode='categorical', # Change if your test set structure is different\n","        shuffle=False,      # No need to shuffle test data\n","        image_size=(img_size, img_size),\n","        batch_size=batch_size\n","    )\n","    return test_ds\n","\n","\n","def configure_datasets(train_ds, val_ds, test_ds=None, augment=True):\n","    \"\"\"\n","    Applies data augmentation and performance optimizations (caching, prefetching).\n","\n","    - Rescaling: Normalizes pixel values from [0, 255] to [0, 1].\n","    - Augmentation: Applies random transformations to the training set.\n","    - Prefetching: Loads the next batch of data while the GPU is busy.\n","    \"\"\"\n","\n","    # Define the Rescaling layer\n","    rescale_layer = tf.keras.layers.Rescaling(1./255)\n","\n","    # Get the augmentation layer\n","    augmentation_layer = build_augmentation_layer()\n","\n","    # --- Apply transformations ---\n","\n","    # Apply rescaling to all datasets\n","    train_ds = train_ds.map(lambda x, y: (rescale_layer(x), y),\n","                            num_parallel_calls=tf.data.AUTOTUNE)\n","    val_ds = val_ds.map(lambda x, y: (rescale_layer(x), y),\n","                          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    # Apply augmentation ONLY to the training dataset\n","    if augment:\n","        train_ds = train_ds.map(lambda x, y: (augmentation_layer(x, training=True), y),\n","                                num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    # Apply prefetching for performance\n","    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    if test_ds:\n","        test_ds = test_ds.map(lambda x, y: (rescale_layer(x), y),\n","                              num_parallel_calls=tf.data.AUTOTUNE)\n","        test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n","        return train_ds, val_ds, test_ds\n","\n","    return train_ds, val_ds\n","\n","\n","def plot_sample_batch(dataset, class_names):\n","    \"\"\"\n","    Visualizes a single batch of images from a dataset.\n","    Useful for checking if data augmentation is working.\n","    \"\"\"\n","    plt.figure(figsize=(10, 10))\n","    for images, labels in dataset.take(1):  # Takes one batch\n","        for i in range(9):  # Display first 9 images\n","            ax = plt.subplot(3, 3, i + 1)\n","            plt.imshow(images[i].numpy().astype(\"uint8\")) # Change to \"float\" if rescaled\n","            # Find the index of the '1' in the one-hot encoded label\n","            label_index = tf.argmax(labels[i])\n","            plt.title(class_names[label_index])\n","            plt.axis(\"off\")\n","    plt.suptitle(\"Sample Batch Visualization\")\n","    plt.show()\n","\n","\n","# --- Main execution block (for testing) ---\n","if __name__ == \"__main__\":\n","    \"\"\"\n","    This block runs ONLY when you execute this script directly\n","    (e.g., `python src/preprocess.py`).\n","    It's useful for testing the script independently.\n","    \"\"\"\n","    print(\"Testing preprocessing script...\")\n","\n","    # 1. Load the data\n","    train_dataset, val_dataset, class_names_list = load_datasets()\n","\n","    # 2. Get a sample batch *before* augmentation to show\n","    # Note: We don't apply rescaling here, just for visualization\n","    print(\"Visualizing a sample batch *before* augmentation and rescaling...\")\n","    plot_sample_batch(train_dataset, class_names_list)\n","\n","    # 3. Configure datasets (apply rescaling and augmentation)\n","    # Set augment=True to see the augmented images\n","    train_dataset, val_dataset = configure_datasets(train_dataset, val_dataset, augment=True)\n","\n","    # 4. Visualize a batch *after* augmentation\n","    # Note: Images will be rescaled (float 0-1), so we just show them.\n","    print(\"Visualizing a sample batch *after* augmentation and rescaling...\")\n","    plt.figure(figsize=(10, 10))\n","    for images, labels in train_dataset.take(1):\n","        for i in range(9):\n","            ax = plt.subplot(3, 3, i + 1)\n","            plt.imshow(images[i].numpy())  # Images are now floats [0, 1]\n","            label_index = tf.argmax(labels[i])\n","            plt.title(class_names_list[label_index])\n","            plt.axis(\"off\")\n","    plt.suptitle(\"Sample Batch (After Augmentation)\")\n","    plt.show()\n","\n","    print(\"\\nPreprocessing script test complete.\")\n","    print(f\"Class Names: {class_names_list}\")\n","    print(f\"Train dataset element spec: {train_dataset.element_spec}\")\n","    print(f\"Validation dataset element spec: {val_dataset.element_spec}\")"],"metadata":{"id":"zYUYp_n7Af-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","# Configuration\n","IMG_SIZE = 256\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","DATA_DIR = \"data/train\"\n","\n","# Load and preprocess data\n","def load_data():\n","    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","        DATA_DIR,\n","        validation_split=0.2,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(IMG_SIZE, IMG_SIZE),\n","        batch_size=BATCH_SIZE\n","    )\n","\n","    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","        DATA_DIR,\n","        validation_split=0.2,\n","        subset=\"validation\",\n","        seed=123,\n","        image_size=(IMG_SIZE, IMG_SIZE),\n","        batch_size=BATCH_SIZE\n","    )\n","\n","    class_names = train_ds.class_names\n","    print(f\"Classes found: {class_names}\")\n","\n","    return train_ds, val_ds, class_names\n","\n","# Create CNN model\n","def create_model(num_classes):\n","    model = keras.Sequential([\n","        layers.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n","\n","        layers.Conv2D(32, 3, activation='relu'),\n","        layers.MaxPooling2D(),\n","\n","        layers.Conv2D(64, 3, activation='relu'),\n","        layers.MaxPooling2D(),\n","\n","        layers.Conv2D(128, 3, activation='relu'),\n","        layers.MaxPooling2D(),\n","\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","\n","    return model\n","\n","# Main training function\n","def main():\n","    print(\"Loading data...\")\n","    train_ds, val_ds, class_names = load_data()\n","\n","    print(\"\\nBuilding model...\")\n","    model = create_model(len(class_names))\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","\n","    print(\"\\nModel Summary:\")\n","    model.summary()\n","\n","    print(\"\\nTraining model...\")\n","    history = model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=EPOCHS\n","    )\n","\n","    # Save model\n","    model.save('models/potato_disease_model.h5')\n","    print(\"\\nModel saved to models/potato_disease_model.h5\")\n","\n","    # Plot training history\n","    plt.figure(figsize=(12, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.title('Model Accuracy')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Model Loss')\n","\n","    plt.tight_layout()\n","    plt.savefig('models/training_history.png')\n","    print(\"Training history plot saved to models/training_history.png\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"foFS-mX1Aqoq"},"execution_count":null,"outputs":[]}]}